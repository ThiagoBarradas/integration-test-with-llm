# ðŸ§ª Integration Test Generation Using LLMs and TSL

This repository contains the artifacts for the study **"Combining TSL and LLMs to Automate REST API Testing: A Comparative Study"**, which evaluates the use of large language models (LLMs) to generate integration tests for REST APIs based on OpenAPI specifications with an intermediate step using TSL.

## ðŸ“‚ Repository Structure

```
â”œâ”€â”€ general-files/
â”‚   â”œâ”€â”€ default-prompts/           # Prompt examples used in the few-shot and decomposed prompting strategy
â”‚   â”œâ”€â”€ examples/                  # OpenAPI, TSL and Integration Tests examples used to teach LLMs 
â”œâ”€â”€ llm-processor/                 # Script for LLM interaction and execution
â”œâ”€â”€ projects/
â”‚   â””â”€â”€ <project-name>/            # Cloned API projects with OpenAPI and results
â”‚       â””â”€â”€ prompt-engineering/
â”‚           â”œâ”€â”€ prompts/           # Input - Specific prompt used in this project
â”‚           â”œâ”€â”€ output/            # Output - Generated test cases and integration test code for each LLM in this project
â”œâ”€â”€ pdfs/                          # Public evidence files (e.g., results summary)
```

## ðŸš€ How to use LLM Processor

Check instructions [here](llm-processor/README.md).

## ðŸ“Š Results

The study evaluated multiple LLMs (GPT 4o (OpenAI), LLaMA 3.2 90b (Meta), Claude 3.5 Sonnet (Anthropic), Gemini 1.5 Pro (Google), Deepseek R1 (Deepseek), Mistral Large (Mistral), Qwen 2.5 32b (Alibaba), and SabiÃ¡ 3 (Maritaca)) using metrics such as:
- âœ… Success Rate
- ðŸ“ˆ Code Coverage
- ðŸ§¬ Mutation Score
- ðŸ’° Cost per Execution

Result can be found in:
- Output: [projects](projects/files.md).
- Metrics: [pdfs](pdfs/all_results.pdf).

## ðŸ“„ Paper

A preprint of the full paper will be made available here upon acceptance.  
In the meantime, the anonymous version used for submission is available at:

ðŸ“Ž [Anonymous Submission (4Open)](https://anonymous.4open.science/r/integration-test-with-llm)